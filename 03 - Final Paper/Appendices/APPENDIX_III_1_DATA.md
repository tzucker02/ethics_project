| Study / Report | Key Findings on ELL Writers | False‑Positive Rate / Metric |
|----------------|-----------------------------|--------------------------|
| **Center for Democracy & Technology brief (based on Stanford study) – “Disproportionate Effects of Generative AI Detectors on English Learners”** | AI detectors that worked almost perfectly on native‑speaker essays **falsely flagged a majority of TOEFL essays** written by non‑native speakers. | **61 %** of non‑native essays flagged; **19 %** unanimously flaged; **97 %** flagged by at least one detector. |
| ***The Markup* (Mathewson, 2023)** | Replicated the Stanford study; **ELL writers were disproportionately likely to be flagged** and several real disciplinary cases were reported. | Approximately **60 %** false‑positive rate (same as the Stanford/​CDT numbers). |
| **Turnitin blog (internal research)** | Acknowledges **bias toward ELL writers**; overall false‑positive rate is **< 1 %** when ≥ 20 % of the text is AI‑generated, but the rate **rises sharply for low‑AI‑content texts**, especially for ELL writers. | No exact percentage given for the low‑AI scenario, but the report confirms a **higher incidence of false positives for ELL writers** compared with native speakers. |
