# AI‑Detectors & English‑Language‑Learners (ELL): Policy Proposal

## 1. Problem Statement & Ethical Context  

Current AI‑detectors systematically disadvantage English‑Language‑Learners (ELL), violating Title IV civil‑rights protections. Institutions therefore need a policy that **couples detector use with mandatory human review, transparency, and an appeals process**.  

Existing guidelines merely warn that AI detectors “should not be the sole basis for determining if a student’s work has been produced with AI assistance” (Dang & Wang, 2024; Najarro, 2023). In practice, there are few, if any, policies that specifically address how instructors should use AI detectors with ELL students.

## 2. Evidence of Disproportionate Impact  

- Multiple education‑forum posts highlight that **false positives are more common with ELL learners** (Hirsch, 2024; Woelfel, 2023).  
- The Stanford study reported **up to a 61 % false‑positive rate for ELL essays** (Furze, 2024; Mathewson, 2023; Myers, 2023; Najarro, 2023).  
- False positives have led to **academic‑integrity investigations, emotional distress, and even visa‑status risks** (Castellanos‑Canales, 2025; University of Rochester, 2025).  

These outcomes conflict with both **deontological duties** (equal treatment of all students under Title IV) and **consequentialist concerns** (wasted faculty time, inflated false‑positive counts, potential legal liability).

## 3. Philosophical Justification  

### 3.1 Deontological Perspective  
An institution has a duty to treat all students as moral equals. Relying on a tool that **systematically disadvantages a protected group** breaches that duty, regardless of overall detector accuracy.

### 3.2 Consequentialist Perspective  
Using AI‑detectors without safeguards creates measurable harms:
- Inflated false‑positive counts (Wood, 2024)  
- Wasted faculty time (Alsharefeen, 2025)  
- Legal liability for civil‑rights violations (McLogan, 2025)  

Thus, the **policy must constrain AI‑detectors with procedural safeguards** (human review, transparent error rates, appeals).

## 4. Why Sole Use of AI‑Detectors Is Problematic  

The very justification for AI‑detectors—to ensure integrity and help students achieve their best work—becomes counterproductive when **false positives force ELL students to defend work they have already done**. This diverts energy from learning to litigation.

## 5. Recommended Policy Framework  

1. **Mandatory Human Review** – Any AI‑detector flag triggers a faculty review before any academic‑integrity action.  
2. **Transparency** – Publish detector error rates, especially false‑positive rates for ELL writing.  
3. **Appeals Process** – Provide a clear, timely mechanism for students to contest AI‑detector results.  
4. **Student Notification** – Inform students in advance about the existence of AI‑detectors, their limitations, and the policy governing their use.  
5. **Baseline Sampling & Initial Scanning** – Use low‑stakes sampling (Cambridge, 2025; Kratzer, 2020) and initial scans (“Assessing Student Writing,” 2025) to identify potential cases before full‑scale detection.

## 6. Legal Considerations  

- **Title IV** protects against disparate treatment and hostile learning environments.  
- False positives that disproportionately affect ELL students could trigger **civil‑rights claims**.  
- Institutions must avoid policies that could lead to **visa‑status revocation** for international students.

## 7. Existing Institutional Practices  

- Several universities have **general AI‑use policies** (BU, 2023; CELT, 2025; Cornell, 2023; Vanderbilt, 2023) but **do not address ELL‑specific concerns**.  
- Some institutions (University of Pittsburgh, 2023; Vanderbilt, 2023) advise **not using AI detectors at all** due to high false‑positive rates.

## 8. Technical Options & Cost Considerations  

| Factor | Questions to Ask |
|--------|------------------|
| **Cost** | Subscription price, free‑trial length, text‑submission limits |
| **Accuracy** | Reported overall accuracy vs. accuracy on ELL writing |
| **Features** | Does it include plagiarism checking? AI‑humanizer? |
| **Integration** | Browser plug‑in, API, LMS integration |
| **Batch Upload** | Ability to process multiple essays at once |

## 9. Implementation Timeline (High‑Level Overview)

| Phase | Duration | Core Goal |
|-------|----------|-----------|
| **0 – Initiation & Governance** | 2 weeks | Secure sponsorship, define scope, assemble steering team |
| **1 – Discovery & Requirements** | 4 weeks | Gather evidence, map workflows, capture stakeholder needs |
| **2 – Policy Draft & Legal Review** | 3 weeks | Write policy, embed bias‑mitigation, obtain legal clearance |
| **3 – Technical Pilot (Semester 1)** | 6 weeks | Test detector + human‑review workflow in a limited setting |
| **4 – Evaluation & Iteration** | 2 weeks (post‑pilot) | Analyse pilot data, refine policy & tech stack |
| **5 – Full‑Scale Roll‑out (Semester 2)** | 8 weeks | Institution‑wide adoption, training, support |
| **6 – Monitoring & Continuous Improvement** | Ongoing (quarterly) | Track compliance, refine processes, report to governance |
| **7 – Formal Close‑out** | 2 weeks (end of year) | Archive documentation, hand‑off to permanent governance |

*(Detailed sub‑tasks, owners, and success metrics are provided in the full timeline appendix.)*

## 10. Stakeholder Analysis  

| Stakeholder | Power | Interest | Recommended Action |
|-------------|-------|----------|---------------------|
| **ELL / International Students** | Low | High | Conduct focus groups; publish plain‑language policy |
| **Faculty** | Medium | High | Draft department‑level addenda; host workshops |
| **Provost / Academic Integrity Office** | High | High | Sponsor steering committee; approve budget |
| **Legal / Compliance Office** | High | Medium‑High | Review policy language for Title IV compliance |
| **IT Services** | High | Medium | Provide API integration, timeline, support |
| **AI‑Detector Vendors** | Medium | Low‑Medium | Request bias‑mitigation documentation |
| **State/Education Agencies** | High | Low‑Medium | Submit compliance summary |
| **Student Government** | Low | High | Invite to policy‑review meetings |
| **General Public / Media** | Low | Low | Prepare press kit on demand |

## 11. Anticipated Critiques & Mitigations  

| Critique | Response |
|----------|----------|
| **Increased faculty workload** | Pilot a **human‑review workflow** that limits reviews to flagged cases; provide **training modules** and **compensation** where appropriate. |
| **Potential re‑introduction of bias via human reviewers** | Implement **bias‑training** for reviewers; use **standardized rubrics** to guide decisions. |
| **Technical feasibility / cost** | Conduct a **cost‑benefit analysis** (e.g., Pangram Labs $5 /student/yr vs. potential legal exposure). |
| **Impact on visa status for international students** | Ensure **transparent appeals** and **legal counsel** involvement before any sanction that could affect immigration status. |
| **Risk of over‑reliance on detectors** | Position detectors as **one indicator** among many (writing samples, drafts, oral defenses). |

## 12. Conclusion  

AI‑detectors, in their current form, **disproportionately penalize ELL students**, creating civil‑rights, legal, and humanitarian concerns. A **policy that couples detector use with mandatory human review, transparent error reporting, and an accessible appeals process** mitigates these harms while preserving the legitimate goal of academic integrity. The phased implementation plan, stakeholder matrix, and mitigation strategies outlined above provide a practical roadmap for institutions to adopt a fair, lawful, and ethically sound approach to AI‑detector use.

---  

*Prepared by Thomas Zucker‑Scharff*  
*Date: November 15, 2025*  
*Course: OMDS DX701*  
*Institution: Boston University*
